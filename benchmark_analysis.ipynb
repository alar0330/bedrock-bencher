{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Data Analysis\n",
    "\n",
    "Quick examples for loading and analyzing benchmark runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bedrock_benchmark.core import BenchmarkCore\n",
    "from bedrock_benchmark.storage import StorageManager\n",
    "\n",
    "# Initialize\n",
    "storage_manager = StorageManager('./experiments')\n",
    "benchmark_core = BenchmarkCore(storage_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Experiments and Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List experiments\n",
    "experiments = benchmark_core.list_experiments()\n",
    "for exp in experiments:\n",
    "    print(f\"{exp.id}: {exp.name} ({len(exp.runs)} runs)\")\n",
    "\n",
    "# Get runs from first experiment\n",
    "if experiments:\n",
    "    experiment_id = experiments[0].id\n",
    "    runs = benchmark_core.list_runs(experiment_id)\n",
    "    print(f\"\\nRuns in {experiment_id}:\")\n",
    "    for run_id in runs:\n",
    "        summary = benchmark_core.get_run_summary(run_id)\n",
    "        print(f\"  {run_id}: {summary['model_id']} - {summary['total_responses']} responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first run into DataFrame\n",
    "if runs:\n",
    "    run_id = runs[0]\n",
    "    df = benchmark_core.export_run_to_dataframe(run_id)\n",
    "    \n",
    "    print(f\"Loaded run {run_id}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"\\nAverage latency: {df['latency_ms'].mean():.2f}ms\")\n",
    "    print(f\"Total tokens: {df['input_tokens'].sum() + df['output_tokens'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Multiple Runs for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple runs (if available)\n",
    "if len(runs) >= 2:\n",
    "    comparison_runs = runs[:2]  # First 2 runs\n",
    "    df_multi = benchmark_core.export_multiple_runs_to_dataframe(comparison_runs)\n",
    "    \n",
    "    print(f\"Loaded {len(comparison_runs)} runs for comparison\")\n",
    "    print(f\"Shape: {df_multi.shape}\")\n",
    "    \n",
    "    # Show run distribution\n",
    "    print(\"\\nRun distribution:\")\n",
    "    print(df_multi['run_id'].value_counts())\n",
    "    \n",
    "    # Compare stats by run\n",
    "    comparison_stats = df_multi.groupby('run_id').agg({\n",
    "        'latency_ms': ['mean', 'std'],\n",
    "        'input_tokens': 'sum',\n",
    "        'output_tokens': 'sum',\n",
    "        'actual_response': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nComparison by run:\")\n",
    "    display(comparison_stats)\n",
    "else:\n",
    "    print(f\"Only {len(runs)} run available - need 2+ for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single run visualization\n",
    "if 'df' in locals() and not df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Latency histogram\n",
    "    ax1.hist(df['latency_ms'], bins=20, alpha=0.7)\n",
    "    ax1.set_title('Latency Distribution')\n",
    "    ax1.set_xlabel('Latency (ms)')\n",
    "    \n",
    "    # Token scatter\n",
    "    ax2.scatter(df['input_tokens'], df['output_tokens'], alpha=0.6)\n",
    "    ax2.set_title('Input vs Output Tokens')\n",
    "    ax2.set_xlabel('Input Tokens')\n",
    "    ax2.set_ylabel('Output Tokens')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-run comparison visualization\n",
    "if 'df_multi' in locals() and not df_multi.empty:\n",
    "    # Latency comparison boxplot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    df_multi.boxplot(column='latency_ms', by='run_id')\n",
    "    plt.title('Latency Comparison by Run')\n",
    "    plt.suptitle('')  # Remove default title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames to CSV\n",
    "if 'df' in locals():\n",
    "    df.to_csv('single_run_analysis.csv', index=False)\n",
    "    print(\"Saved single run data to: single_run_analysis.csv\")\n",
    "\n",
    "if 'df_multi' in locals():\n",
    "    df_multi.to_csv('multi_run_comparison.csv', index=False)\n",
    "    print(\"Saved comparison data to: multi_run_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
